{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcf956c4",
   "metadata": {},
   "source": [
    "# **LlamaIndex / RAG Pipeline Evaluation**\n",
    "**Goal:**\n",
    "Build a RAG Evaluation Notebook that compares:\n",
    "* LangChain RAG (you already built)\n",
    "* LlamaIndex RAG (new)\n",
    "\n",
    "...on the same document using Gemini embeddings + Gemini LLM\n",
    "and evaluates:\n",
    "* Retrieval quality\n",
    "* Latency\n",
    "* Cost (approx / mock)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ca00a7",
   "metadata": {},
   "source": [
    "### Import Libraries & Load API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "101f59b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported and API key configured.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Private\\AI-PROJECTS-PORTFOLIO-DOCS-ASSETS\\ALL-PROJECTS-PACKAGES\\gen-ai-and-llm\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure the Gemini API key\n",
    "import google.generativeai as genai\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found. Please create a .env file with your key.\")\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "print(\"âœ… Libraries imported and API key configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d623f9e8",
   "metadata": {},
   "source": [
    "### Load the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09873276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“˜ Document loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the knowledge base document\n",
    "try:\n",
    "    with open(\"../data/sample_document.txt\", \"r\") as f:\n",
    "        document_text = f.read()\n",
    "    print(\"ðŸ“˜ Document loaded successfully.\")\n",
    "    # print(document_text)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: sample_document.txt not found. Please create it first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f22bf4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded docs: 1\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"../data/\").load_data()\n",
    "print(\"Loaded docs:\", len(documents))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62698c2e",
   "metadata": {},
   "source": [
    "### LangChain RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845952f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LangChain Vector Store created.\n",
      "âœ… LangChain RAG Chain is ready.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# 1. Initialize Gemini Models\n",
    "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "gemini_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.3)\n",
    "\n",
    "# 2. Split the document into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "documents = text_splitter.create_documents([document_text])\n",
    "\n",
    "# 3. Create a Vector Store (FAISS) with Gemini Embeddings\n",
    "try:\n",
    "    langchain_vector_store = FAISS.from_documents(documents, gemini_embeddings)\n",
    "    langchain_retriever = langchain_vector_store.as_retriever()\n",
    "    print(\"âœ… LangChain Vector Store created.\")\n",
    "\n",
    "    # 4. Create the LangChain RetrievalQA chain\n",
    "    langchain_rag_chain = RetrievalQA.from_chain_type(\n",
    "        llm=gemini_llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=langchain_retriever\n",
    "    )\n",
    "    print(\"âœ… LangChain RAG Chain is ready.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e60e1e7",
   "metadata": {},
   "source": [
    "### LlamaIndex RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f466122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LlamaIndex Settings configured for Gemini.\n",
      "âœ… LlamaIndex Vector Store Index created.\n",
      "âœ… LlamaIndex Query Engine is ready.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.embeddings.google import GeminiEmbedding\n",
    "from llama_index.llms.gemini import Gemini\n",
    "\n",
    "\n",
    "# 1. Configure LlamaIndex to use Gemini models\n",
    "# Note: LlamaIndex uses a global Settings object for configuration\n",
    "Settings.llm = Gemini(model_name=\"models/gemini-2.0-flash\")\n",
    "Settings.embed_model = GeminiEmbedding(model_name=\"models/text-embedding-004\")\n",
    "Settings.chunk_size = 500\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "print(\"âœ… LlamaIndex Settings configured for Gemini.\")\n",
    "\n",
    "# 2. Load documents and build the index\n",
    "# LlamaIndex can read directly from a file/directory\n",
    "try:\n",
    "    llama_documents = SimpleDirectoryReader(input_files=[\"../data/sample_document.txt\"]).load_data()\n",
    "    llama_index = VectorStoreIndex.from_documents(llama_documents)\n",
    "    print(\"âœ… LlamaIndex Vector Store Index created.\")\n",
    "\n",
    "    # 3. Create the LlamaIndex Query Engine\n",
    "    llama_query_engine = llama_index.as_query_engine()\n",
    "    print(\"âœ… LlamaIndex Query Engine is ready.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8b116f",
   "metadata": {},
   "source": [
    "### RAG Evaluation Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81846dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Evaluation will be run on 5 questions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define a set of questions to test the RAG pipelines\n",
    "evaluation_questions = [\n",
    "    \"What is the Mediterranean diet?\",\n",
    "    \"What are the main health benefits of this diet?\",\n",
    "    \"What is the primary source of fat mentioned?\",\n",
    "    \"How does the diet help improve cholesterol levels?\",\n",
    "    \"What role do antioxidants play in the Mediterranean diet?\"\n",
    "]\n",
    "\n",
    "print(f\" Evaluation will be run on {len(evaluation_questions)} questions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting evaluation...\n",
      "\n",
      "Evaluating question: 'What is the Mediterranean diet?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_20400\\3929602419.py:15: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  lc_retrieved_docs = langchain_retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating question: 'What are the main health benefits of this diet?'\n",
      "\n",
      "Evaluating question: 'What is the primary source of fat mentioned?'\n",
      "\n",
      "Evaluating question: 'How does the diet help improve cholesterol levels?'\n",
      "\n",
      "Evaluating question: 'What role do antioxidants play in the Mediterranean diet?'\n",
      "\n",
      "âœ… Evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "print(\" Starting evaluation...\")\n",
    "\n",
    "for question in evaluation_questions:\n",
    "    print(f\"\\nEvaluating question: '{question}'\")\n",
    "\n",
    "    # --- LangChain Evaluation ---\n",
    "    lc_start_time = time.time()\n",
    "    lc_response = langchain_rag_chain.invoke({\"query\": question})\n",
    "    lc_end_time = time.time()\n",
    "    lc_latency = lc_end_time - lc_start_time\n",
    "    # Retrieve the source documents used by LangChain\n",
    "    lc_retrieved_docs = langchain_retriever.get_relevant_documents(question)\n",
    "    lc_context = \"\\n\\n\".join([doc.page_content for doc in lc_retrieved_docs])\n",
    "\n",
    "    # --- LlamaIndex Evaluation ---\n",
    "    li_start_time = time.time()\n",
    "    li_response = llama_query_engine.query(question)\n",
    "    li_end_time = time.time()\n",
    "    li_latency = li_end_time - li_start_time\n",
    "    # LlamaIndex response object contains the source nodes\n",
    "    li_context = \"\\n\\n\".join([node.get_content() for node in li_response.source_nodes])\n",
    "\n",
    "    # Store results for this question\n",
    "    results.append({\n",
    "        \"question\": question,\n",
    "        \"langchain_answer\": lc_response['result'],\n",
    "        \"langchain_context\": lc_context,\n",
    "        \"langchain_latency\": lc_latency,\n",
    "        \"llamaindex_answer\": str(li_response),\n",
    "        \"llamaindex_context\": li_context,\n",
    "        \"llamaindex_latency\": li_latency\n",
    "    })\n",
    "\n",
    "print(\"\\nâœ… Evaluation complete.\")\n",
    "\n",
    "# Convert results to a pandas DataFrame for easier analysis\n",
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c7f23e",
   "metadata": {},
   "source": [
    "### Evaluate Quality with LLM-as-a-Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897fc727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during evaluation: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 15\n",
      "Please retry in 54.154815166s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 54\n",
      "}\n",
      "]\n",
      "Error during evaluation: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 15\n",
      "Please retry in 53.616961094s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 53\n",
      "}\n",
      "]\n",
      "Error during evaluation: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 15\n",
      "Please retry in 53.197460649s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 53\n",
      "}\n",
      "]\n",
      "Error during evaluation: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 15\n",
      "Please retry in 52.622609044s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 52\n",
      "}\n",
      "]\n",
      "Error during evaluation: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 15\n",
      "Please retry in 52.208174481s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 52\n",
      "}\n",
      "]\n",
      "âœ… Quality scores have been calculated.\n"
     ]
    }
   ],
   "source": [
    "from google.generativeai.types import GenerationConfig\n",
    "\n",
    "# This is a simple LLM-as-a-judge prompt template\n",
    "EVALUATION_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an impartial AI judge. Evaluate the quality of a generated answer based on a given context and question.\n",
    "Your evaluation should be a score from 1 to 5, where 5 is the best.\n",
    "Do not provide any explanation, just the integer score.\n",
    "\n",
    "**Question:**\n",
    "{question}\n",
    "\n",
    "**Retrieved Context:**\n",
    "---\n",
    "{context}\n",
    "---\n",
    "\n",
    "**Generated Answer:**\n",
    "---\n",
    "{answer}\n",
    "---\n",
    "\n",
    "**Evaluation Criteria:**\n",
    "1.  **Relevance**: Does the context contain the information needed to answer the question?\n",
    "2.  **Faithfulness**: Is the answer fully supported by the provided context? There should be no hallucinations.\n",
    "3.  **Conciseness**: Is the answer concise and to the point?\n",
    "\n",
    "**Your Score (1-5):**\n",
    "\"\"\"\n",
    "\n",
    "evaluation_llm = genai.GenerativeModel('gemini-2.0-flash')\n",
    "config = GenerationConfig(temperature=0.0) # Set temp to 0 for deterministic scoring\n",
    "\n",
    "def evaluate_response(question, context, answer):\n",
    "    \"\"\"Uses Gemini to evaluate the quality of a RAG response.\"\"\"\n",
    "    prompt = EVALUATION_PROMPT_TEMPLATE.format(question=question, context=context, answer=answer)\n",
    "    try:\n",
    "        response = evaluation_llm.generate_content(prompt, generation_config=config)\n",
    "        # Safely parse the score\n",
    "        score = int(response.text.strip())\n",
    "        return score\n",
    "    except (ValueError, IndexError):\n",
    "        # Handle cases where the LLM doesn't return a clean integer\n",
    "        return 0 # Return a default score on failure\n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Apply the evaluation function to our results\n",
    "df_results['langchain_score'] = df_results.apply(\n",
    "    lambda row: evaluate_response(row['question'], row['langchain_context'], row['langchain_answer']), axis=1\n",
    ")\n",
    "\n",
    "df_results['llamaindex_score'] = df_results.apply(\n",
    "    lambda row: evaluate_response(row['question'], row['llamaindex_context'], row['llamaindex_answer']), axis=1\n",
    ")\n",
    "\n",
    "print(\"âœ… Quality scores have been calculated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969485a6",
   "metadata": {},
   "source": [
    "### Display Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10385c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ðŸ“Š Evaluation Results ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>langchain_score</th>\n",
       "      <th>llamaindex_score</th>\n",
       "      <th>langchain_latency</th>\n",
       "      <th>llamaindex_latency</th>\n",
       "      <th>langchain_answer</th>\n",
       "      <th>llamaindex_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the Mediterranean diet?</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2.622672</td>\n",
       "      <td>2.015608</td>\n",
       "      <td>The Mediterranean diet is a heart-healthy, pla...</td>\n",
       "      <td>It is a heart-healthy, plant-based eating plan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the main health benefits of this diet?</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2.555379</td>\n",
       "      <td>2.332010</td>\n",
       "      <td>The Mediterranean diet has several health bene...</td>\n",
       "      <td>This eating plan is associated with a lower ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the primary source of fat mentioned?</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.369761</td>\n",
       "      <td>1.463135</td>\n",
       "      <td>Olive oil is the primary source of fat mention...</td>\n",
       "      <td>Olive oil is the primary source of fat mention...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does the diet help improve cholesterol lev...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.877143</td>\n",
       "      <td>2.013202</td>\n",
       "      <td>The Mediterranean diet's emphasis on healthy f...</td>\n",
       "      <td>The diet's focus on healthy fats, such as thos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What role do antioxidants play in the Mediterr...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2.072845</td>\n",
       "      <td>2.325169</td>\n",
       "      <td>Antioxidants play a key role in the Mediterran...</td>\n",
       "      <td>Antioxidants, abundant in fresh produce, play ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  langchain_score  \\\n",
       "0                    What is the Mediterranean diet?                5   \n",
       "1    What are the main health benefits of this diet?                5   \n",
       "2       What is the primary source of fat mentioned?                5   \n",
       "3  How does the diet help improve cholesterol lev...                5   \n",
       "4  What role do antioxidants play in the Mediterr...                5   \n",
       "\n",
       "   llamaindex_score  langchain_latency  llamaindex_latency  \\\n",
       "0                 0           2.622672            2.015608   \n",
       "1                 0           2.555379            2.332010   \n",
       "2                 0           1.369761            1.463135   \n",
       "3                 0           1.877143            2.013202   \n",
       "4                 0           2.072845            2.325169   \n",
       "\n",
       "                                    langchain_answer  \\\n",
       "0  The Mediterranean diet is a heart-healthy, pla...   \n",
       "1  The Mediterranean diet has several health bene...   \n",
       "2  Olive oil is the primary source of fat mention...   \n",
       "3  The Mediterranean diet's emphasis on healthy f...   \n",
       "4  Antioxidants play a key role in the Mediterran...   \n",
       "\n",
       "                                   llamaindex_answer  \n",
       "0  It is a heart-healthy, plant-based eating plan...  \n",
       "1  This eating plan is associated with a lower ri...  \n",
       "2  Olive oil is the primary source of fat mention...  \n",
       "3  The diet's focus on healthy fats, such as thos...  \n",
       "4  Antioxidants, abundant in fresh produce, play ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ðŸ“ˆ Average Metrics ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Framework</th>\n",
       "      <th>Avg. Quality Score (1-5)</th>\n",
       "      <th>Avg. Latency (seconds)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LangChain</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LlamaIndex</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Framework Avg. Quality Score (1-5) Avg. Latency (seconds)\n",
       "0   LangChain                     5.00                   2.10\n",
       "1  LlamaIndex                     0.00                   2.03"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For a cleaner final view, let's select and reorder columns\n",
    "final_df = df_results[[\n",
    "    'question',\n",
    "    'langchain_score',\n",
    "    'llamaindex_score',\n",
    "    'langchain_latency',\n",
    "    'llamaindex_latency',\n",
    "    'langchain_answer',\n",
    "    'llamaindex_answer'\n",
    "]]\n",
    "\n",
    "# Calculate averages\n",
    "avg_lc_score = final_df['langchain_score'].mean()\n",
    "avg_li_score = final_df['llamaindex_score'].mean()\n",
    "avg_lc_latency = final_df['langchain_latency'].mean()\n",
    "avg_li_latency = final_df['llamaindex_latency'].mean()\n",
    "\n",
    "print(\"--- ðŸ“Š Evaluation Results ---\")\n",
    "display(final_df)\n",
    "\n",
    "print(\"\\n--- ðŸ“ˆ Average Metrics ---\")\n",
    "summary_data = {\n",
    "    \"Framework\": [\"LangChain\", \"LlamaIndex\"],\n",
    "    \"Avg. Quality Score (1-5)\": [f\"{avg_lc_score:.2f}\", f\"{avg_li_score:.2f}\"],\n",
    "    \"Avg. Latency (seconds)\": [f\"{avg_lc_latency:.2f}\", f\"{avg_li_latency:.2f}\"]\n",
    "}\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0221b7f6",
   "metadata": {},
   "source": [
    "### Mock Cost Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4e2da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ðŸ’° Estimated Costs ---\n",
      "One-time Embedding Cost: $0.000067\n",
      "Total LangChain Query Cost: $0.000971\n",
      "Total LlamaIndex Query Cost: $0.000677\n"
     ]
    }
   ],
   "source": [
    "def estimate_cost(text_input, text_output, is_embedding=False):\n",
    "    input_chars = len(text_input)\n",
    "    output_chars = len(text_output)\n",
    "\n",
    "    if is_embedding:\n",
    "        return (input_chars / 1000) * 0.0001\n",
    "\n",
    "    input_cost = (input_chars / 1000) * 0.000125\n",
    "    output_cost = (output_chars / 1000) * 0.000375\n",
    "    return input_cost + output_cost\n",
    "\n",
    "# 1. Embedding cost (one-time)\n",
    "embedding_cost = estimate_cost(document_text, \"\", is_embedding=True)\n",
    "\n",
    "# 2. Querying cost (per query)\n",
    "lc_query_cost = df_results.apply(\n",
    "    lambda row: estimate_cost(row['question'] + row['langchain_context'], row['langchain_answer']), axis=1\n",
    ").sum()\n",
    "\n",
    "li_query_cost = df_results.apply(\n",
    "    lambda row: estimate_cost(row['question'] + row['llamaindex_context'], row['llamaindex_answer']), axis=1\n",
    ").sum()\n",
    "\n",
    "print(\"--- ðŸ’° Estimated Costs ---\")\n",
    "print(f\"One-time Embedding Cost: ${embedding_cost:.6f}\")\n",
    "print(f\"Total LangChain Query Cost: ${lc_query_cost:.6f}\")\n",
    "print(f\"Total LlamaIndex Query Cost: ${li_query_cost:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0374ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
