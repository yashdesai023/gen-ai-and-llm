{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ea4a9e6",
   "metadata": {},
   "source": [
    "## **DocChat RAG Bot — LangChain + Chroma + Gemini**\n",
    "\n",
    "**Goal:**\n",
    "Create an intelligent chatbot that can answer questions based on any uploaded PDF or text document — powered by LangChain, Gemini embeddings, and a vector database (Chroma or FAISS)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2eedbc",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5147bc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Private\\AI-PROJECTS-PORTFOLIO-DOCS-ASSETS\\ALL-PROJECTS-PACKAGES\\gen-ai-and-llm\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from dotenv import load_dotenv\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aeb2ff",
   "metadata": {},
   "source": [
    "### Set Up Gemini API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddc25364",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6438c307",
   "metadata": {},
   "source": [
    "### Load Your Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "873ea440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2651aee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(data,\n",
    "                    glob=\"*.pdf\",\n",
    "                    loader_cls=PyPDFLoader)\n",
    "    \n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7efbe133",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf(\"../data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8316d1d4",
   "metadata": {},
   "source": [
    "### Split Text into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "526a734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 20)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82520e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of my chunk: 2524\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_chunks = text_split(extracted_data)\n",
    "print(\"length of my chunk:\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e87eca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08ffd32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5c0901f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, query_encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57b3b3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c876cdf",
   "metadata": {},
   "source": [
    "### Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7d85587",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78237bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(\"rag-chatbot-economy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6a06a654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the same model you used to create your index's data\n",
    "# Example: 'all-MiniLM-L6-v2' has a dimension of 384\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "\n",
    "# Your query text\n",
    "query_text = \"Tell me about UPSCprep\"\n",
    "\n",
    "# Generate the embedding. This will be a list of 384 numbers.\n",
    "query_embedding = model.encode(query_text).tolist() \n",
    "\n",
    "# print(len(query_embedding)) # This should print 384\n",
    "# print(query_embedding[:5])  # Look at the first few numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "362bc282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.4454\n",
      "Text: Weekly current affairs sessions.\n",
      "Prelims + Mains coverage \n",
      "(Basics to Advanced)\n",
      "Prelims and Mains Test Series\n",
      "with evaluation.\n",
      "UPSC ESSENTIALS \n",
      "+ MENTORSHIP 2024\n",
      "Enroll Now : courses.upscprep.com\n",
      "₹ 18499/-PRICE\n",
      "S im p lify  y o u r  U P S C  jo u r n e y  \n",
      "w ith  e x p e r t m e n to r s\n",
      " A b h ije e t Y a d a v , C o -F o u n d e r\n",
      "( A IR  6 5 3 , C S E  2 0 17  )\n",
      "Adv.Shashank Ratnoo, Co-Founder \n",
      "( AIR 688, CSE 2015 )\n",
      "--------------------\n",
      "Score: 0.4454\n",
      "Text: Weekly current affairs sessions.\n",
      "Prelims + Mains coverage \n",
      "(Basics to Advanced)\n",
      "Prelims and Mains Test Series\n",
      "with evaluation.\n",
      "UPSC ESSENTIALS \n",
      "+ MENTORSHIP 2024\n",
      "Enroll Now : courses.upscprep.com\n",
      "₹ 18499/-PRICE\n",
      "S im p lify  y o u r  U P S C  jo u r n e y  \n",
      "w ith  e x p e r t m e n to r s\n",
      " A b h ije e t Y a d a v , C o -F o u n d e r\n",
      "( A IR  6 5 3 , C S E  2 0 17  )\n",
      "Adv.Shashank Ratnoo, Co-Founder \n",
      "( AIR 688, CSE 2015 )\n",
      "--------------------\n",
      "Score: 0.3780\n",
      "Text: and knowledge economy. This includes initiatives to improve digital infrastructure and conne ctivity, \n",
      "promote digital literacy, and develop e-governance systems. \n",
      "2. The Aadhaar digital identity system, launched in 2009, provides citizens with a unique 12 -digit \n",
      "identification number that can be used to access a range of government services and benefits. \n",
      "3. The Unified Payments Interface (UPI) system, launched in 2016, allows citizens to make instant and\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "query_response = index.query(\n",
    "    vector=query_embedding,\n",
    "    top_k=3,\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "# 3. Print the results\n",
    "for match in query_response['matches']:\n",
    "    print(f\"Score: {match['score']:.4f}\")\n",
    "    if 'text' in match['metadata']:\n",
    "        print(f\"Text: {match['metadata']['text']}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c90ff68",
   "metadata": {},
   "source": [
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunks,\n",
    "    embedding=embeddings,\n",
    "    index_name=\"rag-chatbot-economy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74016c9",
   "metadata": {},
   "source": [
    "### RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37c08f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import Pinecone as PineconeVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d487097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "71a19216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from pinecone import Pinecone\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "PINECONE_INDEX_NAME = \"rag-chatbot-economy\"\n",
    "# Important: Use the same model you used for creating embeddings in your DB\n",
    "EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e912b250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully connected to Pinecone index.\n",
      "{'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {'': {'vector_count': 2524}},\n",
      " 'total_vector_count': 2524,\n",
      " 'vector_type': 'dense'}\n",
      "✅ Embedding model loaded successfully.\n",
      "✅ Gemini model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Pinecone\n",
    "try:\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    index = pc.Index(PINECONE_INDEX_NAME)\n",
    "    print(\"✅ Successfully connected to Pinecone index.\")\n",
    "    print(index.describe_index_stats())\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error connecting to Pinecone: {e}\")\n",
    "\n",
    "# Initialize the embedding model\n",
    "try:\n",
    "    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "    print(\"✅ Embedding model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading embedding model: {e}\")\n",
    "\n",
    "# Initialize the Gemini model\n",
    "try:\n",
    "    genai.configure(api_key=GOOGLE_API_KEY)\n",
    "    llm = genai.GenerativeModel('gemini-2.0-flash')\n",
    "    print(\"✅ Gemini model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error configuring Gemini: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5bbd7c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rag_response(user_query):\n",
    "    \"\"\"\n",
    "    Takes a user query and returns a response from Gemini based on Pinecone context.\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve from Pinecone\n",
    "    query_embedding = embedding_model.encode(user_query).tolist()\n",
    "    query_results = index.query(\n",
    "        vector=query_embedding,\n",
    "        top_k=3,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    \n",
    "    context_chunks = [match['metadata']['text'] for match in query_results['matches']]\n",
    "    context_string = \"\\n\\n\".join(context_chunks)\n",
    "\n",
    "    # Step 2: Augment the Prompt for Gemini\n",
    "    prompt_template = f\"\"\"\n",
    "    You are a helpful assistant for the UPSC exam. \n",
    "    You are a professional and helpful assistant. Your goal is to provide clear, concise, and well-formatted answers based on the provided context.\n",
    "\n",
    "    Please answer the user's question using only the information given in the following context.\n",
    "    If the information is not available in the context, politely state that you don't have enough information to answer.\n",
    "\n",
    "    When you formulate the answer, follow these rules:\n",
    "    - Synthesize the information into a helpful response. Do not just copy-paste from the context.\n",
    "    - Use bullet points for lists or to break down key points.\n",
    "    - Use bolding to highlight important terms.\n",
    "    - Maintain a positive and professional tone.\"\n",
    "    CONTEXT:\n",
    "    {context_string}\n",
    "\n",
    "    QUESTION:\n",
    "    {user_query}\n",
    "\n",
    "    ANSWER:\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 3: Generate the Response with Gemini\n",
    "    try:\n",
    "        response = llm.generate_content(prompt_template)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred while generating the response: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "16f03175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- UPSC Chatbot ---\n",
      "Ask a question about the documents you've stored. Type 'exit' to quit.\n",
      "\n",
      "AI: **Club goods** are those that are **excludable** but **non-rivalrous**.\n",
      "\n",
      "Here's a breakdown of the key characteristics:\n",
      "\n",
      "*   **Excludable:** Access can be restricted to those who pay or meet membership criteria.\n",
      "*   **Non-rivalrous:** One person's consumption does not reduce the availability of the good for others.\n",
      "\n",
      "Some examples of club goods are:\n",
      "\n",
      "*   Cable television\n",
      "*   Private golf courses\n",
      "*   Subscription-based services like Netflix\n",
      "\n",
      "\n",
      "AI: Here's a breakdown of the types of goods discussed:\n",
      "\n",
      "*   **Consumer Goods:** These are purchased by individuals or households for personal use to fulfill needs and desires. Examples include food, clothing, electronics, and furniture.\n",
      "*   **Capital Goods:** These are used by businesses to produce other goods or provide services, and are not for direct consumption.\n",
      "*   **Common Goods:** These are **rivalrous** but **non-excludable**, meaning that while anyone can use them, one person's use reduces the amount available for others. Examples include fisheries, forests, and grazing lands.\n",
      "\n",
      "\n",
      "AI: Based on the text, the correct answer is **(c) Rivalrous but Non-excludable**.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "*   **Rivalrous:** Consumption of the good by one person reduces its availability for others.\n",
      "*   **Non-excludable:** It is not possible to prevent others from using or consuming the good.\n",
      "\n",
      "\n",
      "AI: The correct option is **(a) Currency in circulation + demand deposits held by commercial banks**.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "*   **M1** is the narrowest measure of money supply in India.\n",
      "*   **M1** includes currency in circulation and demand deposits held by commercial banks.\n",
      "\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- UPSC Chatbot ---\")\n",
    "print(\"Ask a question about the documents you've stored. Type 'exit' to quit.\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nYou: \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    # Get the response from our RAG function\n",
    "    ai_response = get_rag_response(user_input)\n",
    "    print(f\"\\nAI: {ai_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71daad1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
